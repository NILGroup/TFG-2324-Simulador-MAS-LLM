\chapter{Planteamiento de la Solución}
\label{cap:planteamiento}
\section{Planteamiento inicial}

Una vez vistas las diferencias esenciales entre el sistema original que hemos heredado y las ampliaciones que hemos realizado durante el desarrollo del presente trabajo, se tratará las ideas iniciales de cómo pensábamos realizar el trabajo, indicando desde el punto de vista técnico, organizativo y general, la manera de afrontar este proyecto.

En cuanto a la organización, la planificación inicial era clara: invertir la mayor parte de los primeros meses investigando el proyecto, y a partir del final del primer cuatrimestre y durante todo el segundo cuatrimestre, realizar el trabajo de desarrollo completo del proyecto.

Esta primera fase de investigación fue larga y tediosa, ya que el tema de este TFG fue propuesto por los alumnos, sin una idea clara sobre qué objetivos podría tener el presente trabajo, pero sabiendo que el tema de las simulaciones multi agente es un campo muy interesante en el que profundizar. Durante estos primeros meses se realizaron lluvias de ideas y propuestas para los objetivos. Algunos de los primeros objetivos consistían en añadir funcionalidades a la aplicación o permitir a los usuarios modificarla más, pero decidimos pivotar para poder hacer la aplicación ya existente, más fácil de usar para perfiles no técnicos, así como centrarnos en extender ciertas funcionalidades para que sean más útiles para este tipo de perfiles, en lugar para programadores.

Entre las complejidades que se encontraron durante esta primera fase de investigación, se destacan las inconsistencias entre versiones de ciertas tecnologías, la comprensión de todo el código existente, o el aprendizaje de las tecnologías usadas en el desarrollo del proyecto, entre otras muchas. Además, al investigar sobre el tema, esta fue la fase en la que se desarrolló el capítulo del Estado de la Cuestión, lo que ayudó a tomar otra perspectiva acerca del proyecto.

Durante la segunda fase, una vez definidos y validados los primeros objetivos, e comenzó con todo el desarrollo del sistema. Caba destacar que en esta sección, muchas reconsideraciones tuvieron que ser realizadas, además de multitud de cambios en el código original como adaptaciones para que funcionen los nuevos cambios. Todo esto está explicado en las siguientes secciones.

En el transcurso de ambas fases, tuvimos reuniones regularmente con nuestros tutores, supervisando el estado del sistema, así como los avances, problemas y consideraciones que tuviéramos que tener en cuenta. En un principio estas reuniones eran más espaciadas en el tiempo, haciendo una o dos al mes, y en la fase de desarrollo, donde avanzábamos más rápido, las reuniones eran semanales o cada dos semanas para estar al día del proyecto.



\subsection{Estado tecnológico inicial del sistema}

En esta sección se analizará, a grandes rasgos, el estado inicial desde el punto de vista técnico del proyecto. Es decir, el diseño y situación que había inicialmente para ejecutar el sistema, para que en las próximas secciones se entiendan los cambios realizados y el porqué de estos.

En cuanto al diseño, originalmente, existían 2 backends. El primero de ellos era el backend de Django, el cual era el encargado de gestionar todas las llamadas del frontend, o interfaz, que inicialmente no eran muchas, ya que el frontend solamente servía como apoyo para poder visualizar las simulaciones. 

El segundo backend era el de reverie, como lo llaman los autores del código. Este se encargaba de procesar todos los comandos que los usuarios deseaban ejecutar, calculaba el resultado de estos y se comunicaba con el backend de django para que procesase el resultado de las ejecuciones y se reflejara en la interfaz.

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.9\textwidth]{Imagenes/Vectorial/disenoSistemaOriginal.jpeg}
	\caption{Diseño del sistema original}
	\label{fig:sistemaOriginal}
\end{figure}

Como se puede ver en la figura \ref{fig:sistemaOriginal}, se ve reflejado el estado del sistema desde el punto del diseño. El flujo habitual sería el siguiente:

\begin{enumerate}
	\item El usuario iría a la terminal en la que se está ejecutando el backend de reverie (que gestiona todas las llamadas a los modelos de lenguaje) y ejecutaría alguno de los comandos (run, whisper, exit, save...)
	
	\item El backend de reverie realizaría la serie de llamadas al modelo de lenguaje pertinentes, así como la organización de las respuestas y almacenamiento de las mismas, y pasaría el resultado de la ejecución al backend de django
	
	\item El backend de Django, con la nueva información obtenida de reverie, adaptaría la información a un formato para que pueda ser procesado en el frontend
	
	\item Se procesaría la información de django en el frontend y se enseñaría al usuario final. Como se ha mencionado, el usuario no podría interactuar con esta información para realizar llamadas al backend desde la interfaz, sino que tendría que acudir a la terminal de reverie para comunicarse
	
\end{enumerate}

También es importante resaltar que para realizar acciones como crear una simulación, ver una simulación existente o interactuar con ella, todas las acciones debían ser realizadas mediante la terminal. La interfaz estaba relegada únicamente a mostrar el resultado de la simulación.

\section{Ajustes y reconsideraciones}

En esta sección se tratarán las adaptaciones que se han ido adoptando durante el desarrollo del proyecto, indicando así cómo han ido cambiando distintas facetas del sistema, tanto en su planificación como en su implementación técnica.

Este análisis permitirá comprender mejor la evolución del proyecto, los desafíos enfrentados y las soluciones implementadas. Además, revelará la capacidad de adaptación y el aprendizaje continuo que han sido fundamentales para el éxito del proyecto.

\subsection{Cambios en los objetivos}

Inicialmente, se fijaron una serie de objetivos sin tener un conocimiento claro sobre la arquiterctura y funcionamiento del sistema. Estos objetivos estaban enfocados en la extensibilidad y modificación de lo previamente existente. A grandes rasgos, los objetivos inicialmente marcados eran los siguientes:

\begin{enumerate}
	\item Añadir uno o varios mapas complementarios a mayores del existente, además de poder modificar las apariencias, nombres y personalidades de los agentes de la simulación
	
	\item Permitir la interacción directa con los agentes y el entorno, pudiendo clicar directamente sobre el mapa y que se reconociese el objeto que se estaba indicando, para así poder modificar su estado (indicar que se está quemando el objeto, por ejemplo)
	
	\item Permitir a los usuarios utilizar su propio modelo del lenguaje, ya sea descargado en local o empleando diferentes APIs externas alternativas al modelo del lenguaje original
\end{enumerate}

Tras evaluar todo el sistema, se decidió pivotar sobre estos objetivos iniciales, ya que conllevaban una gran cantidad de trabajo y apenas aportaban valor añadido al programa original, por lo que se propusieron unos nuevos objetivos, sobre los cuales se ha construído la base de los objetivos finales que se pueden consultar en la introducción de este documento. El resultado de pivotar los objetivos fue, en líneas generales, el siguiente:

\begin{enumerate}
	\item Modificación de las personalidades de los personajes, enfocándose así en la importancia de crear nuevas simulaciones con distintas relaciones sociales, siendo agnósticos al mapa e imagen de los agentes, ya que cambiar esto no aporta tanto valor
	
	\item Interacción en tiempo real con los agentes, mediante un menú en el que poder entrevistar al personaje, así como crearle inquietudes y nuevas memorias para ver cómo reacciona
	
	\item Generación automática de resúmenes
	
	\item Englobar todas las funcionalidades en una interfaz sencilla e intuitiva
\end{enumerate}

\subsection{Limpieza de archivos del repositorio}
\label{limpiezaArchivos}

Al haber heredado el sistema original, realizado por terceros, pudimos apreciar que este, además de no ser extensible, tenía multitud de carpetas, archivos y funciones que no estaban siendo utilizados a día de hoy para nada en el proyecto, o que habían sido utilizados únicamente para depurar el código y lo habían olvidado. Por ello, decidimos limpiar todos estos datos innecesarios y dejar el sistema más limpio y sencillo, para que sea más sencilla la extensibilidad del mismo en el futuro.

En esta sección hemos recogido algunas de estas piezas de código que hemos eliminado, explicando qué hacían y el porqué de su eliminación:

\begin{itemize}
	\item \textbf{Carpeta \textit{path\_tester}:} Contenía un archivo como el main\_script.html y una vista especial que los desarrolladores usaron para depurar el código de las colisiones de los personajes con los bordes del mapa. No tenía utilidad ahora, solo a la hora de depurar esto en su momento
	
	\item \textbf{Función \textit{replay} del archivo \textit{views.py}:} Era una función que gestionaba la visualización de repeticiones de simulaciones. Estas eran similares a la demo de las simulaciones, pero con propósito de depuración de la vista de ejecución de simulación, por lo que no tenía ya valor
	
	\item \textbf{Carpeta \textit{persona\_state} y funciones asociadas:} En esta carpeta estaban los datos para poder ver en tiempo real todos los parámetros que afectaban a cada uno de los personajes. Esta información solo era usado en las repeticiones, que eliminamos del sistema
	
	\item \textbf{Entornos virtuales preexistentes:} Los desarrolladores de la aplicación inicial, dejaron sus propios entornos virtuales que habían usado para la creación de este proyecto. Como es recomendable que cada persona instale su entorno virtual particular, decidimos eliminar estas carpetas ya que no estaban siendo utilizadas para nada
	
	\item \textbf{Ficheros \textit{admin.py, models.py y tests.py} de la carpeta translator:} Ya que estos eran archivos vacíos que no estaban siendo utilizados,  necesarios para la configuración inicial pero obsoletos en este punto
	
\end{itemize}

\subsection{Selección del modelo de lenguaje}\label{subsec:Selección del modelo de lenguaje}

Para el correcto funcionamiento de SimulAgents es necesario contar con un LLM adecuado. Permitiendo, en el mejor de los casos, una ejecucion rápida, barata y con una calidad generativa considerable.

Desafortunadamente, el desarrollo de LLMs, en cotas tan altas como las actuales, es reciente, por lo que el acceso a esta tecnología, en pleno desarrollo, es limitado.

Partimos del uso de GPT-3.5 que, para una simulacion de 25 agentes y dos dias, implicó un coste de miles de dolares en créditos de tokens para la API, además de varios dias para completar la simulación (se puede consultar en \cite{park2023generative}).

SimulAgents pretende ofrecer al público general este tipo de simulaciones.
Por ello el LLM a usar es un punto fundamental ya que implica un gran compromiso entre Tiempo de simulacion, Calidad generativa y Coste.

Al tomar en consideración la fase de desarrollo de SimulAgents también se nos volvía necesario contar con un LLM con el que poner a prueba las funcionalidades que fueramos integrando.
Esto nos llevó a buscar alternativas OpenSource, ya que no se cuenta con apoyo financiero por parte de la universidad.
Las alternativas encontradas no cumplían con lo mínimo para funcionar en SimulAgents

Por ello también consideramos otras alternativas a OpenAI, que, por motivos que detallaré a continuación, tampoco se adaptaban a nuestras necesidades.

Lo siguiente son las alternativas que consideramos y los problemas que acarreaban:

\subsubsection{Alternativas OpenSource}

El alto coste que supuso la simulación citada anteriormente nos planteó el uso de otros LLM, por lo menos durante el desarrollo de SimulAgents.

\textbf{Llama 2}

La primera de las opciones a la que recurrimos fue el, en su momento, recién publicado modelo Llama 2. El principal motivo para probar este modelo fue sus grandes capacidades generativas, comparables a las de GPT 3, aún con un tamaño 10 veces menor.

El  objetivo de las evaluaciones que iba a realizar era valorar la idoneidad de los posibles modelos para el uso durante el desarrollo  de la aplicación. Esto implicaba el uso de los Modelos en el equipo con el que haría el desarrollo, por lo que las pruebas se realizaron en el mismo.

Para poner a pruebo el modelo con nuestros requisitos específicos utilizamos la herramienta Llama.cpp, la cual nos permitía interactuar con el modelo Llama 2 por medio de un chat de forma local.

Debido a los requisitos de memoria de los modelos de Llama 2, el único que pude ejecutar de forma local fue el modelo Llama-2-7b-chat. El modelo lo obtuve por medio del acceso que ofrece \href{https://llama.meta.com/llama-downloads/}{Meta}. Tras esto cloné el repositorio \href{https://github.com/ggerganov/llama.cpp}{ggerganov/llama.cpp} y seguí los pasos para poder ejecutar un ejemplo de prueba, donde ya se veía una generación de Tokens muy lenta.

Una vez con el modelo y llama.cpp listos elaboré un par de inputs y outputs, generados por la aplicación SimulAgents, para comparar el output que obtendría del modelo Llama 2. Lo primero que quería comprobar era la capacidad de Llama 2 para seguir la estructura necesaria en este tipo de consultas.

Input de ejemplo:

Outputs de ejemplo

[Image Izq output Querido] [Imagen derecha output de Llama 2]

Como se puede observar Llama 2 no era capaz de seguir la estructura requerida, además de un tiempo de inferencia completamente insostenible (alrededor de 3 tokens por segundo) para una única consulta, necesitando en nuestra aplicación consultas como estas por centenas en cada timestep.

Por lo que decidimos ignorar esta opción.

\textbf{TinyStories}

Posteriormente nos encontramos con cierto trabajo que nos volvió a abrir la posibilidad de usar modelos de forma local y con una inferencia, debido a su tamaño, varios ordenes de magnitud, más pequeño que modelos como los de Llama 2.

El trabajo TinyStories \cite{eldan2023tinystories} propone modelos, con tamaños entre 1 y 28 millones de parámetros, enfocados en generar historias, que lograban un uso de la gramática y sintáxis excelente. Por lo que pensamos que unido a una capacidad de generar la información de forma estructurada podrían ser una alternativa considerable.

Por ello también pusimos a prueba a estos modelos, pero enseguida se notó una falta considerable de recursos semánticos, que no sería posible suplir por medio de un fine-tuning, como habíamos pensado.

[Ejemplos de conversaciones con los distintos modelos]

 \textbf{Quantization}

El último de los de usar un LLM de forma local fue tras el conocimiento de las técnicas de Cuantización, que permitían usar modelos con tamaños mucho menores y cierto deterioro, pero que quizá podía ser asumible.

Volvimos a probar Llama 2, pero en esta ocasión con la versión de 13b.

[Mostrar imágenes de Outputs obtenidos con distintas cuantizaciones de Llama 13b]

Sin embargo, los tiempos de inferencia siguieron siendo grandes y las repsuestas obtenidas tampoco mejoran demasiado. Por lo que decidimos movernos al uso de APIs de terceros como las de OpenAI o Google.

\subsubsection{Otras APIs}

Con los problemas que hemos visto en la sección anterior finalmente acabamos decidiendo volver a los Modelos de terceros, de esta forma lograríamos un tiempo de inferencia mucho menor, un umbral menor para el numero de usuarios que puedieran usar SimulAgents (evitar la carga de memoria que suponía un LLM) y una calidad de generación garantizada al ser modelos mucho más grandes y capaces.

Entre las alternativas posibles, la más inmediata era volver al uso de GPT 3.5, pero decidimos probar antes con otras APIs como la de PaLM.

\textbf{PaLM}

Esta opción prometía tener unos resultados similares a los de GPT-3.5 en cuanto a velocidad y, si no igual almenos lo suficientemente cercana, en cuanto a calidad, o esto parecía en la interacción a través del portal web de Google para interactuar con PaLM. Además de esto también nos resultó útil el acceso a la API con un total de 100 dolares de peticiones iniciales a la API. Que planeabamos usar durante la fase del desarrollo.

Por ello procedimos con la adaptación de las peticiones al LLM para que hiciese las peticiones a la API de google, en lugar de la de OpenAI. Esto quedó reflejado en los archivos gpt\_structure\_palm.py y run\_gpt\_prompt.py del repositorio, adaptados a partir de gpt\_structure.py y run\_gpt\_prompt.py que ya se usaban.

La primera de las modificaciones fué el uso de las librerías de esta API, en concreto google.generativeai, y tras esto adaptar las peticiones al LLM usando las funciones definidas en la \href{https://ai.google.dev/palm_docs}{API de Google}.

El modelo que usamos para las peticiones a la API de Google fue \textit{text-bison-001} y tras la adaptación de las llamadas comprobamos que, a pesar de que fuese capaz de seguir la estructura requerida en las respuestas, no era capaz de seguir un formato uniforme en todas ellas, dando lugar a errores, además de esto, era bastante menos elocuente a la hora de generar acciones para lo agentes. Y esto unido a la necesidad de una VPN para poder usar, debido a que España estaba entre los paises vetados para el uso de esta API, nos hicieron por decantarnos por GPT-3.5 que ya tenía una eficacia comprobada.

\textbf{OpenAI}

Tras la vuelta al uso de GPT-3.5 y la solución de diversos problemas, relativos a las plantillas de prompt y que venían del repositorio tal y como estaba implementado, lo primero que vimos fué que a la hora de ejecutar el proyecto al completo contabamos con una cantidad de rate limits en varios de los tipos de solicitudes que hacíamos.

Estos iban en función del tipo de usuario, usando un sistema de Tiers en el que ascendias según la cantidad de uso que habías hecho de la API, uso que se traducía en dinero. La limitación que nos suponía a la hora de interactuar con el modelo a través de la aplicación era enorme. La cantidad de peticiones que se hacían a la API por cada Agente por step de simulación eran más de 100. Y la restricción que teníamos en la API era de 2 requests por minuto.

Debido a esto optamos por avanzar en otro tipo de problemas de nuestro proyecto, como el front o la adaptación del backend a una interacción por medio de una UI.

Este parón se vió finalizado con las sucesivas actualizaciones que OpenAI hizo de su API y políticas, llegando el momento en que ya podíamos hacer uso de la API, debido al aumento del Rate Limit y además el abaratamiento de la API.

En este momento decidimos retomar el uso de la API y enlazarlo ya finalmente con el resto del proyecto. Sin embargo estas actualizaciones vinieron acompañadas de fuertes cambios en la implementación de la API. Lo que implicó realizar una migración del mismo a la versión más reciente de la API para poder utilizarla. La actualización de la API fue de la versión 0.27.0 a la versión 1.13.3. 

Tras migrar correctamente la versión de la API se consiguió ejecutar satisfactoriamente la totalidad del proyecto haciendo uso del LLM. Sin embargo la velocidad no era, ni mucho menos, a tiempo real, la cantidad de peticiones que se hacian era grande, al rededor de 2 centenas por Agente por timestep. Vimos que había varias fases de la simulación que se podían optimizar, ejecutando en una única consulta más de una petición que hiciera SimulAgents y así reducir significativamente el tiempo de simulación. Este tipo de mejoras plantemos implementarlas en caso de que nos diese tiempo.

\section{Problemas encontrados}

En esta sección se tratarán algunos de los problemas surgidos durante el desarrollo de todo el proyecto. Teniendo en cuenta el planteamiento inicial, y debido a que el tema tratado en este trabajo es una investigación novedosa implementada por terceros, surgieron varios problemas e imprevistos que tuvimos que superar y resolver.

\subsection{Llamadas al back end durante la simulación}

Uno de los principales problemas con el que nos encontramos fueron las dificultades de hacer llamadas al back end mientras se ejecutaba una simulación. Esto era debido a que, mientras una simulación se ejecuta, en la interfaz se puede ver el mapa con todos los personajes, ejecutando las acciones en cada momento. Este mapa se renderiza y ejecuta utilizando la tecnología de Phaser, un framework usado para desarrollar videojuegos usando HTML.

Al usar este framework, como utiliza los botones del teclado para interactuar con el mapa (como las flechas para mover la cámara por el mapa, por ejemplo), uno de los botones que están "reservados" por Phaser es el de la barra espaciadora, lo que hacía que no se pudiesen enviar susurros, ya que no permitía el sistema que se insertaran espacios.

Tras investigar y dar con la raíz del problema, se abrían varias opciones para solucionarlo. La primera era crear una nueva página solamente para los susurros, donde no interactuase con el motor de Phaser y así poder enviar los mensajes. Esta solución era simple y fácil de implementar, pero llegamos a la conclusión de que no tendría demasiado sentido, ya que crear una nueva vista únicamente para enviar un mensaje (sin esperar una respuesta por parte del personaje) no era ideal, ya que la experiencia de usuario cobra más sentido cuando puedes enviar el susurro mediante la interfaz de la simulación, abriendo un modal.

La segunda opción, y la que decidimos implementar finalmente, fue la de intentar pausar la ejecución de la simulación para que así Phaser "libere" los recursos y permita a los usuarios usar la barra espaciadora. Esto nos resultó con mayor sentido ya que así el usuario no tendría que estar constantemente cambiand entre vistas y resultaría en una mejor experiencia. Por tanto, implementamos unos botones para pausar y reanudar la simulación, que cuando se pulsan activan y desactivan otros ciertos botones. Por ejemplo, cuando se reanuda la simulación, no se permite al usuario chatear o susurrar con los personajes (se desactivan estos botones), pero cuando se pausa la simulación, el botón que se desactiva es el de ejecutar steps de la simulación.

\subsection{Acoplo de los personajes al mapa original}
\label{problemaPersonajes}
Como se ha mencionado en varias ocasiones, el sistema original no estaba pensado para ser extensible en multitud de aspectos. Uno de los aspectos más importantes es que todos los personajes tenían nombres, apariencias y personalidades predefinidas, lo que no permitía a los usuarios modificar los personajes y sus personalidades, por tanto, se verían encorsetados a siempre utilizar los mismos personajes con las mismas relaciones entre ellos y generar experimentos a partir de esto, lo cual limita muchísimo el uso de esta aplicación.

Uno de los mayores retos que nos propusimos fue despegar a los personajes de sus personalidades inherentes, para así poder hacer que los usuarios elijan las personalidades de cada uno y ver así los fenómenos sociales que aparecían.

Al no estar inicialmente pensado el sistema para esto, todos los nombres de los personajes, sus pensamientos, experiencias y objetos en el mapa, estaban directamente "hardcodeados" en el programa. Es decir, que estaban escritos directamente y no se permitía que se cambiasen.

El mayor problema fue hacer que los personajes fuesen agnósticos del mapa en el que se encontraban. El mapa inicial trataba de simular una villa, por lo que cada casa y cada objeto tenía el nombre de la familia o personaje que fuese su dueño, lo cual hacía muy fácil la interacción con los objetos de los distintos personajes. Nuestra solución fue que los personajes mantuviesen los nombres originales, pero que eso no influyese en la simulación, ya que ahora no importarían los objetos dentro de la simulación y los usuarios podrían decidir el contexto en el que se encuentran los persoajes, por lo que el mapa servirá ahora simplemente como un contenedor para visualizar la simulación.

\subsection{Problema instalación de requerimientos del proyecto}

Al inicio del proyecto, en la primera fase de investigación de la herramienta, instalamos el proyecto en nuestros dispositivos personales y hubieron algunos problemas. El más importante de estos problemas era que inicialmente, el archivo \textquotesingle requirements.txt\textquotesingle no funcionaba para los Mac con procesador M1.

Tras encontrarnos con este fallo, por problemas de dependencias, estuvimos estancados durante unas semanas, ya que no podíamos ejecutar el sistema y estaba habiendo problemas para solucionarlo por todos los medios.

Primeramente, instalamos un entorno virtual, con la versión de Python exacta que recomendaban los desarrolladores (la 3.9.12) y esto no resultó positivamente. También intentamos resolver las dependencias manualmente, pero surgían otros errores de dependencias al hacer esto, y nunca conseguimos resolverlas todas.

Tras estos fallos y bloqueos, decidimos preguntar a los tutores del TFG para ver qué soluciones alternativas podríamos encontrar, pero no llegamos a una conclusión. 

Tras semanas intentando resolver este problema y avanzando en el trabajo por otras vías (escribiendo la memoria, investigando la arquitectura y diseño del sistema...), encontramos que los propios desarrolladores habían publicado un patch en el que se solucionaba este problema, actualizando así el archivo requirements.txt. Al actualizar e implementar este cambio, las dependencias se resolvieron y pudimos comenzar con el trabajo e investigación práctica, con el sistema corriendo en nuestros dispositivos locales y habiendo avanzado en otras materias.

\section{Conclusión final}

Tras ver el estado inicial en el que se encontraba el sistema, tanto desde un punto de vista técnico como desde un punto de vista más general, y teniendo en cuenta todos los problemas encontrados, realizando las reconsideraciones y ajustes necesarios, se termina con un sistema más robusto, usable y amigable.

Por tanto, en la figura \ref{fig:sistemaFinal}, se aprecia, a grandes rasgos, cómo ha cambiado el sistema desde el punto de vista del diseño a alto nivel, llevando a cabo los cambios previamente mencionados en las secciones pasadas.

\begin{figure}[h]
	\centering
	\includegraphics[width = 0.9\textwidth]{Imagenes/Vectorial/disenoSistemaFinal.jpeg}
	\caption{Diseño del sistema final tras las adaptaciones}
	\label{fig:sistemaFinal}
\end{figure}

Ahora, como se puede ver, hay un punto único de interacción entre el usuario y el sistema, que es mediante la interfaz. Así, interactuando con la interfaz los usuarios son capaces ahora de hacer lo mismo que antes y más, con las extensiones implementadas.

Mediante esta interacción, los usuarios pueden ejecutar todos los comandos, ver las simulaciones, crearlas e interactuar con ellas libremente.

El flujo de ejecución ahora es algo diferente:

\begin{enumerate}
	\item El usuario interactúa mediante la interfaz decidiendo la acción que desee
	
	\item Si el usuario desea ejecutar un comando de la simulación, se captará mediante la interfaz, se redirigirá al backend de django, el cual gestionará la llamada al gestor de comandos (nos permite ejecutar comandos mientras la aplicación sigue funcionando) y esta se encargará de llamar al backend de reverie para que ejecute el comando
	
	\item El backend de reverie devolverá la ejecución del comando a Django
	
	\item Al igual que antes, Django procesará esta información y la devolverá a la interfaz, donde el usuario podrá visualizarla y volver a interactuar con esta nueva información
	
\end{enumerate}
