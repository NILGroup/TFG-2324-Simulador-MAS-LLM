\chapter{Planteamiento de la Solución}
\label{cap:planteamiento}
\section{Planteamiento inicial}

\subsection{Estado tecnológico inicial del sistema}

\section{Ajustes y reconsideraciones}

En esta sección se tratarán las adaptaciones que se han ido realizando a lo largo del desarrollo del proyecto, indicando así cómo han ido cambiando distintas facetas del sistema, tanto en su planificación como en su implementación técnica.

Este análisis permitirá comprender mejor la evolución del proyecto, los desafíos enfrentados y las soluciones implementadas. Además, revelará la capacidad de adaptación y el aprendizaje continuo que han sido fundamentales para el éxito del proyecto.

\subsection{Cambios en los objetivos}

Durante el desarrollo del presente trabajo, nos hemos propuesto una serie de objetivos los cuales hemos ido modificando a medida que se progresaba.

Inicialmente, se fijaron una serie de objetivos sin tener un conocimiento claro sobre la arquiterctura y funcionamiento del sistema. Estos objetivos estaban enfocados en la extensibilidad y modificación de lo previamente existente. A grandes rasgos, los objetivos inicialmente marcados eran los siguientes:

\begin{enumerate}
	\item Añadir uno o varios mapas complementarios a mayores del existente, además de poder modificar las apariencias, nombres y personalidades de los agentes de la simulación.
	
	\item Permitir la interacción directa con los agentes y el entorno, pudiendo clicar directamente sobre el mapa y que se reconociese el objeto que se estaba indicando, para así poder modificar su estado (indicar que se está quemando el objeto, por ejemplo)
	
	\item Permitir a los usuarios utilizar su propio modelo del lenguaje, ya sea descargado en local o empleando diferentes APIs externas alternativas al modelo del lenguaje original.
\end{enumerate}

Tras evaluar todo el sistema, se decidió pivotar sobre estos objetivos iniciales, ya que conllevaban una gran cantidad de trabajo y apenas aportaban valor añadido al programa original, por lo que se propusieron unos nuevos objetivos, sobre los cuales se ha construído la base de los objetivos finales que se pueden consultar en la introducción de este documento. El resultado de pivotar los objetivos fue, en líneas generales, el siguiente:

\begin{enumerate}
	\item Modificación de las personalidades de los personajes, enfocándose así en la importancia de crear nuevas simulaciones con distintas relaciones sociales, siendo agnósticos al mapa e imagen de los agentes, ya que cambiar esto no aporta tanto valor.
	
	\item Interacción en tiempo real con los agentes, mediante un menú en el que poder entrevistar al personaje, así como crearle inquietudes y nuevas memorias para ver cómo reacciona.
	
	\item Generación automática de resúmenes
	
	\item Englobar todas las funcionalidades en una interfaz sencilla e intuitiva
\end{enumerate}

\subsection{Limpieza de archivos del repositorio}

\subsection{Selección del modelo de lenguaje}

---Estructura actual-----

Para el correcto funcionamiento de SimulAgents es necesario contar con un LLM adecuado. Permitiendo, en el mejor de los casos, una ejecucion rápida, barata y con una calidad generativa considerable.

Desafortunadamente, el desarrollo LLMs en cotas tan altas como las actuales es reciente, por lo que el acceso a esta tecnología, en pleno desarrollo, es limitado.

Partimos del uso de GPT-3.5 que, para una simulacion de 25 agentes y dos dias, implicó un coste de miles de dolares en créditos de tokens y varios dias para completar la simulación(citado en el paper de Generative Agents Apartado 8.2 Parrafo 1).

SimulAgents pretende ofrecer al público general este tipo de simulaciones.
Por ello el LLM a usar es un punto fundamental ya que implica un gran compromiso entre Tiempo de simulacion, Calidad generativa y Coste.

Al tomar en consideración la fase de desarrollo de SimulAgents también se nos volvía necesario contar con un LLM con el que poner a prueba las funcionalidades añadidas.
Esto nos llevó a buscar alternativas OpenSource, ya que no se cuenta con apoyo financiero por parte de la universidad.
Las alternativas encontradas no cumplian con lo mínimo para funcionar en SimulAgents

Por ello también consideramos otras alternativas a OpenAI, que por diversos motivos tampoco se adaptaban a nuestras necesidades.

A continuación detallaremos las alternativas que consideramos y los problemas que acarreaban.

\subsubsection{Alternativas OpenSource}
 - Tiny Histories
 - Llama
 - Quantization
\subsubsection{Otras APIs}
 - PaLM
 - Vuelta a OpenAI
 	- Rate limits muy restrictivos -> Stand by
 	- Asumimos usar GPT-3.5
 	 - Cambio en la version de la API que conlleva la adaptación a la nueva API
 	 - Todo funciona a excepción de ciertos errores en templates ofrecidos por Paper
 	 - La velocidad de ejecución es muy limitada
 	 - Proponemos ofrecer acceso a varias APIs

---Estructura actual-----

------Datos antiguos-----
La herramienta que plantea desarrollar este trabajo requiere del uso de un LLM, tanto para alguna fase del desarrollo como para hacer uso del trabajo final. Actualmente este tipo de herramientas, a pesar de estar viviendo de los mayores avances, no está disponible ampliamente debido a los requisitos técnicos que suponen estos modelos. Esta restricción surge principalmente por el tamaño que tienen los modelos, ofreciendo dos alternativas. La primera de ellas es el uso de APIs como la de OpenAI, obligandote a depender de terceros en la generación de lenguaje. Sin embargo sigue siendo de las mejores opciones, debido a que la alternativa restante es emplear modelos Open Source de forma local, lo cual es posible, pero complicado, ya que los modelos capaces de realizar las tareas necesarias 
Desde paper vimos costes de decenas de miles de \$
Para herramienta como la que hemos propuesto -> Necesario un Modelo
Durante el desarrollo Sería necesario en funcionalidades extendidas - lo más inmediato sería continuar con GPT3.5
	Obj 3 -> Resumen que señale los instantes más relevantes de la simulación
Para la herramienta final sería necesario proporcionar soporte para diferentes APIs o incluso para LLMs locales

En primer lugar comentaremos lo relativo al modelo usado para el desarrollo
	Problema inicial
	No había aporte monetario de ningun tipo por parte de la facultad en cuanto al uso de APIs como la de OpenAI
	Primera alternativa -> Uso de Modelos OpenSource (Llama 2 de varios tamaños)
	Primer problema -> Requisitos Hardware privativos -> Demasiada memoria virtual necesaria
		Dependía del tamaño del modelo, en el caso de Llama variaba desde el 70B hasta 7B requiriendo de 140GB a 14GB de memoria 
		Lo que se vuelve inabarcable para muchos dispositivos incluso con el modelo mas sencillo
	Segunda alternativa -> https://huggingface.co/roneneldan/TinyStories-33M
		Se encuentra el paper TinyStories: How Small Can Language Models Be and Still Speak Coherent English?
		En el que proponen entrenar modelos de tamaños muchísimo menores a los tamaños de Modelos competitivos como GPT4 o BERT
		A pesar de su incomparable tamaño, proponiendo modelos de entre 1 y 28 Millones de parámetros, (3 ordenes de magnitud por debajo de los SOA LLMs), conseguian resultados con un mínimo razonamiento y casi perfecta gramática. Llegando a superar a las variantes de menor tamaño de estos LLMs, estando aún así 3 ordenes de magnitud por encima.
		Por ello se consideraron como una primera alternativa, debido a su pequeño tamaño tendrían una inferencia bastante mas rapido ademas de ser soportable por una gran cantidad de dispositivos
		Además se podria plantear realizar fine tuning para ampliar vocabulario o aumentar la creatividad
	Problemas de Tiny Stories
		Con los modelos más pequeños (1-8.3M parameters) se obtenian resultados incoherentes en cualquier tipo de prompt que usase nuestro sistema
		Con los modelos mas grandes se veian mejores resultados pero de ninguna forma se lograba hacer que siguieran algún tipo de estructura.
		A esto habría que añadirle su pequeña ventana de contexto, de 512 bytes
	
	No se pueden usar estos modelos, demasiadas limitaciones
	Volvemos a enfocarnos en Llama
		Encontramos https://github.com/ggerganov/llama.cpp?tab=readme-ov-file
		Que ofrece una forma de ejecutar multitud de modelos con distintos quantizations, permitiendo ejecutar incluso Llama2 13B en un CPU con 16GB de RAM
	Problemas con Llama
		Sin embargo el tiempo de inferencia es alto, de 3 Tokens por segundo ~= 2 palabras por segundo
		Los resultados tampoco son muy coherentes debido a la perdida de fidelidad al cuantizar los modelos

	En ese entonces sale PaLM, Probamos a usar PaLM
		Adaptamos el sistema para que funcione con la API de PaLM
		La API no ofrece servicio en España
		Con VPN hay mucha latencia ~= de 2s + tpo inferencia por query
		Los resultados tampoco son capaces de seguir los Prompts diseñados

	Solo nos queda probar con GPT3.5 a pesar de los costes que suponga
		Debido a los Rate Limits que imponen es imposible ejecutarlo, debido a los modelos usados por la arquitectura, y el user tier que teniamos estabamos con limites de 2 request por minuto, llegando a enviar 800 request por timestep por agente.
	
	Debido a esas complicaciones el uso de un LLM durante el desarrollo y como alternativa para el usuario final quedó en Stand By
		Se dejó como opción el dejarlo con soporte para varias apis, con el prerquisito de que el usuario de la API tuviera suficientes permisos como para hacer frente a las limitaciones con que nos encontramos en su momento

------Datos antiguos-----

\section{Problemas encontrados}

\section{Conclusión final}
